{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDZ0dIzsfUjX"
      },
      "source": [
        "# End to end patch classifier with KDM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3Pr_YJnfZuC"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOFN7hCeQ2IZ",
        "outputId": "c40ed2ae-3c86-4239-ff52-0bb2f9a2d1bc"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHQHAECwfcYY"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okI4JwtTRDR_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import wandb\n",
        "import sys\n",
        "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "from keras import optimizers\n",
        "from keras import losses\n",
        "from keras import metrics\n",
        "from tensorflow.keras.layers import Conv2D, Resizing, InputLayer, Flatten, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Input, Dense\n",
        "from keras.models import Model\n",
        "import keras\n",
        "import tensorflow_probability as tfp\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import numpy as np\n",
        "import tensorflow_addons as tfa\n",
        "import os\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from collections import OrderedDict\n",
        "import pathlib\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.gaussian_process.kernels import RBF, ConstantKernel\n",
        "from sklearn.preprocessing import MinMaxScaler, normalize\n",
        "from matplotlib import pyplot as pl\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, mean_absolute_error, mean_squared_error, r2_score, mean_squared_log_error, recall_score, f1_score, accuracy_score, cohen_kappa_score, precision_score\n",
        "import glob\n",
        "from sklearn.metrics import pairwise_distances\n",
        "tfd = tfp.distributions\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "CUDA_VISIBLE_DEVICES=\"0,1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gge1NlEVg-A2"
      },
      "source": [
        "## Data from WSI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulNBjGd4RPbZ"
      },
      "outputs": [],
      "source": [
        "# !wandb login c020039f1e51c657e83b9990a5f67a34b5b38a68\n",
        "# wandb.init(project=\"KDM Patch classifier end-to-end\")\n",
        "data_dir = '/data/KDM/data'\n",
        "image_folder = f'{data_dir}/train_images/'\n",
        "mask_folder = f'{data_dir}/train_label_masks/'\n",
        "patches_folder = f'{data_dir}/patches'\n",
        "train_csv = f'{data_dir}/df_train.csv'\n",
        "val_csv = f'{data_dir}/df_val.csv'\n",
        "test_csv = f'{data_dir}/df_test.csv'\n",
        "df_train = pd.read_csv(train_csv)\n",
        "df_val = pd.read_csv(val_csv)\n",
        "df_test = pd.read_csv(test_csv)\n",
        "df_train['gleason_score'] -= 1\n",
        "df_val['gleason_score'] -= 1\n",
        "df_test['gleason_score'] -= 1\n",
        "\n",
        "#WSI\n",
        "df_train_wsi = pd.read_csv(f\"{data_dir}/wsi_train.csv\")\n",
        "df_val_wsi = pd.read_csv(f\"{data_dir}/wsi_val.csv\")\n",
        "df_test_wsi = pd.read_csv(f\"{data_dir}/wsi_test.csv\")\n",
        "train_dict = df_train_wsi.set_index('image_id')['isup_grade'].to_dict(into=OrderedDict)\n",
        "val_dict = df_val_wsi.set_index('image_id')['isup_grade'].to_dict(into=OrderedDict)\n",
        "test_dict = df_test_wsi.set_index('image_id')['isup_grade'].to_dict(into=OrderedDict)\n",
        "train_dict = {k: v for k, v in train_dict.items() if os.path.isfile(os.path.join(f'{data_dir}/tile_mosaics',k+'.jpeg'))}\n",
        "val_dict = {k: v for k, v in val_dict.items() if os.path.isfile(os.path.join(f'{data_dir}/tile_mosaics',k+'.jpeg'))}\n",
        "test_dict = {k: v for k, v in test_dict.items() if os.path.isfile(os.path.join(f'{data_dir}/tile_mosaics',k+'.jpeg'))}\n",
        "del train_dict['1c36b3db47d83f1436bd260288c5723f']\n",
        "del train_dict['50203fbd5de280144cbb16749814a3fe']\n",
        "del test_dict['ecae863e7c478594aa4c84ce132b3825']\n",
        "train_features = list(train_dict.keys())\n",
        "train_labels = np.array(list(train_dict.values()))\n",
        "val_features = list(val_dict.keys())\n",
        "val_labels = np.array(list(val_dict.values()))\n",
        "test_features = list(test_dict.keys())\n",
        "test_labels = np.array(list(test_dict.values()))\n",
        "train_paths = [os.path.join(f'{data_dir}/tile_mosaics',train_path+'.jpeg') for train_path in train_features]\n",
        "val_paths = [os.path.join(f'{data_dir}/tile_mosaics',val_path+'.jpeg') for val_path in val_features]\n",
        "test_paths = [os.path.join(f'{data_dir}/tile_mosaics',test_path+'.jpeg') for test_path in test_features]\n",
        "encoder = OneHotEncoder()\n",
        "y_train_onehot = encoder.fit_transform(train_labels.reshape(-1,1))\n",
        "y_train_one_hot = y_train_onehot.toarray()\n",
        "y_val_onehot = encoder.fit_transform(val_labels.reshape(-1,1))\n",
        "y_val_one_hot = y_val_onehot.toarray()\n",
        "y_test_onehot = encoder.fit_transform(test_labels.reshape(-1,1))\n",
        "y_test_one_hot = y_test_onehot.toarray()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7Kofg7pfnh7"
      },
      "source": [
        "## Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcHuv1aTRjaD"
      },
      "outputs": [],
      "source": [
        "tiles_path = f'{data_dir}/tiles'\n",
        "\n",
        "def decode_img(img):\n",
        "  # Convert the compressed string to a 3D uint8 tensor\n",
        "  img = tf.io.decode_jpeg(img, channels=3)\n",
        "  # Resize the image to the desired size\n",
        "  return img\n",
        "\n",
        "def process_path(file_path):\n",
        "\n",
        "  # Load the raw data from the file as a string\n",
        "  img = tf.io.read_file(file_path)\n",
        "  img = decode_img(img)\n",
        "  return img\n",
        "\n",
        "def create_dataset(df, data_dir, shuffle=False, batch_size=32):\n",
        "\n",
        "  patches_path = df['image_id'].apply(lambda x: os.path.join(data_dir, x.split('_')[0], x + '.jpeg'))\n",
        "  patches_labels = keras.utils.to_categorical(df['gleason_score'], num_classes = 5)\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((patches_path, patches_labels))\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size=len(df))\n",
        "  dataset = dataset.map(lambda x, y: (process_path(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset\n",
        "\n",
        "def create_regression_dataset(df, data_dir, shuffle=False, batch_size=32):\n",
        "\n",
        "  patches_path = df['image_id'].apply(lambda x: os.path.join(data_dir, x.split('_')[0], x + '.jpeg'))\n",
        "  patches_labels = keras.utils.to_categorical(df['gleason_score'], num_classes = 5)\n",
        "  regression_labels = df['gleason_score'] / 4\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((patches_path, regression_labels))\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(buffer_size=len(df))\n",
        "  dataset = dataset.map(lambda x, y: (process_path(x), y), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "  dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset\n",
        "\n",
        "class WsiDataset(tf.keras.utils.Sequence):\n",
        "\n",
        "    def __init__(self, wsi_names, stage, tiles_path, labels, shuffle=True):\n",
        "      self.wsi_names = wsi_names\n",
        "      self.stage = stage\n",
        "      self.tiles_path = tiles_path\n",
        "      self.labels = labels\n",
        "      self.shuffle = shuffle\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wsi_names)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      wsi = self.wsi_names[idx]\n",
        "      y = self.labels[idx]\n",
        "      tile_paths = [os.path.join(tiles_path,f'{self.stage}_tiles', wsi, tile) for tile in os.listdir(os.path.join(self.tiles_path,f'{self.stage}_tiles',wsi))]\n",
        "      tile_matrix = []\n",
        "      for tile in tile_paths:\n",
        "        tile_matrix.append(process_path(tile))\n",
        "      tile_matrix = np.array(tile_matrix)\n",
        "      return tile_matrix, y\n",
        "\n",
        "      def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "class Prototypes(tf.keras.utils.Sequence):\n",
        "    def __init__(self, wsi_dataset):\n",
        "        self.wsi_dataset = wsi_dataset\n",
        "        self.num_classes = 6\n",
        "        self.num_samples_per_class = 36\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.wsi_dataset)\n",
        "\n",
        "    def __getitem__(self, idx): # Determine the sample index within the class\n",
        "\n",
        "        sample, label = self.wsi_dataset[idx]\n",
        "        return sample, np.tile(label, (self.num_samples_per_class, 1))\n",
        "\n",
        "def get_prototypes():\n",
        "\n",
        "  \"\"\"\n",
        "    Retrieves prototypes from a WsiDataset object based on their labels.\n",
        "\n",
        "    Returns:\n",
        "        protos (ndarray): Stacked array of prototypes with shape (N, H, W, C),\n",
        "            where N is the total number of prototypes, H is the height of each prototype,\n",
        "            W is the width of each prototype, and C is the number of channels.\n",
        "        label_protos (ndarray): Stacked array of labels corresponding to the prototypes\n",
        "            with shape (N, L), where N is the total number of prototypes and L is the number\n",
        "            of label dimensions.\n",
        "\n",
        "    Raises:\n",
        "        None\n",
        "\n",
        "    Usage:\n",
        "        protos, label_protos = get_prototypes()\n",
        "    \"\"\"\n",
        "  prototypes = Prototypes(WsiDataset(train_features, \"train\", tiles_path, y_train_one_hot, shuffle=True))\n",
        "  protos = []\n",
        "  label_protos = []\n",
        "  for item in tqdm(range(6)):\n",
        "    for sample, label in prototypes:\n",
        "      if np.argmax(label[0]) == item:\n",
        "          protos.append(sample)\n",
        "          label_protos.append(label)\n",
        "          break\n",
        "  protos = np.concatenate(protos, axis=0)\n",
        "  label_protos = np.concatenate(label_protos, axis=0)\n",
        "  return protos, label_protos\n",
        "\n",
        "def get_patch_prototypes():\n",
        "  \"\"\"\n",
        "    Retrieves prototypes from a dataset object based on their labels.\n",
        "\n",
        "    Returns:\n",
        "        protos (ndarray): Stacked array of prototypes with shape (N, H, W, C),\n",
        "            where N is the total number of prototypes, H is the height of each prototype,\n",
        "            W is the width of each prototype, and C is the number of channels.\n",
        "        label_protos (ndarray): Stacked array of labels corresponding to the prototypes\n",
        "            with shape (N, L), where N is the total number of prototypes and L is the number\n",
        "            of label dimensions.\n",
        "\n",
        "    Raises:\n",
        "        None\n",
        "\n",
        "    Usage:\n",
        "        protos, label_protos = get_prototypes()\n",
        "    \"\"\"\n",
        "  prototypes = create_dataset(df_train, patches_folder, shuffle=True, batch_size=1)\n",
        "  #prototypes = Prototypes(WsiDataset(train_features, \"train\", tiles_path, y_train_one_hot, shuffle=True))\n",
        "  protos = []\n",
        "  label_protos = []\n",
        "  for proto in tqdm(range(216//5)):\n",
        "    for item in range(5):\n",
        "      for sample, label in prototypes:\n",
        "        if np.argmax(label[0]) == item:\n",
        "            protos.append(sample)\n",
        "            label_protos.append(label)\n",
        "            break\n",
        "  for sample, label in prototypes:\n",
        "    protos.append(sample)\n",
        "    label_protos.append(label)\n",
        "    break\n",
        "  protos = np.concatenate(protos, axis=0)\n",
        "  label_protos = np.concatenate(label_protos, axis=0)\n",
        "  return protos, label_protos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pFxGl7Hf9Ci"
      },
      "source": [
        "### Train, Val, Test patch datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mBlfLXRkjCH"
      },
      "outputs": [],
      "source": [
        "train_dataset = create_regression_dataset(df_train, patches_folder, shuffle=True, batch_size=64)\n",
        "val_dataset = create_regression_dataset(df_val, patches_folder, shuffle=False, batch_size=64)\n",
        "test_dataset = create_regression_dataset(df_test, patches_folder, shuffle=False, batch_size=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B68W24ZkgBCy"
      },
      "source": [
        "### Compute class weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM2Si2l6Rxdx"
      },
      "outputs": [],
      "source": [
        "class_sample_count = np.unique(df_train['gleason_score'].to_numpy())\n",
        "class_weights = compute_class_weight(class_weight='balanced',classes=class_sample_count,y=df_train['gleason_score'].to_numpy())\n",
        "print(class_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSrOsHa2c47b"
      },
      "source": [
        "## KDM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regression layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProbRegression(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Calculates the expected value and variance of a measure on a\n",
        "    density matrix. The measure associates evenly distributed values\n",
        "    between 0 and 1 to the different n basis states.\n",
        "    Input shape:\n",
        "        A tensor with shape (batch_size, n)\n",
        "    Output shape:\n",
        "        (batch_size, n, 2)\n",
        "    Arguments:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if len(input_shape) != 2 :\n",
        "            raise ValueError('A `DensityMatrix2Dist` layer should be '\n",
        "                             'called with a tensor of shape '\n",
        "                             '(batch_size, n)')\n",
        "        self.vals = tf.constant(tf.linspace(0.0, 1.0, input_shape[1]), dtype=tf.float32)\n",
        "        self.vals2 = self.vals ** 2\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs.shape) != 2:\n",
        "            raise ValueError('A `DensityMatrix2Dist` layer should be '\n",
        "                             'called with a tensor of shape '\n",
        "                             '(batch_size, n, )')\n",
        "        mean = tf.einsum('...i,i->...', inputs, self.vals, optimize='optimal')\n",
        "        mean2 = tf.einsum('...i,i->...', inputs, self.vals2, optimize='optimal')\n",
        "        var = mean2 - mean ** 2\n",
        "        return tf.stack([mean, var], axis = -1)\n",
        "        #return mean\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[1], 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjeJK98Ec6Pi"
      },
      "outputs": [],
      "source": [
        "def dm2comp(dm):\n",
        "    '''\n",
        "    Extract vectors and weights from a factorized density matrix representation\n",
        "    Arguments:\n",
        "     dm: tensor of shape (bs, n, d + 1)\n",
        "    Returns:\n",
        "     w: tensor of shape (bs, n)\n",
        "     v: tensor of shape (bs, n, d)\n",
        "    '''\n",
        "    return dm[:, :, 0], dm[:, :, 1:]\n",
        "\n",
        "def comp2dm(w, v):\n",
        "    '''\n",
        "    Construct a factorized density matrix from vectors and weights\n",
        "    Arguments:\n",
        "     w: tensor of shape (bs, n)\n",
        "     v: tensor of shape (bs, n, d)\n",
        "    Returns:\n",
        "     dm: tensor of shape (bs, n, d + 1)\n",
        "    '''\n",
        "    return tf.concat((w[:, :, tf.newaxis], v), axis=2)\n",
        "\n",
        "def samples2dm(samples):\n",
        "    '''\n",
        "    Construct a factorized density matrix from a batch of samples\n",
        "    each sample will have the same weight. Samples that are all\n",
        "    zero will be ignored.\n",
        "    Arguments:\n",
        "        samples: tensor of shape (bs, n, d)\n",
        "    Returns:\n",
        "        dm: tensor of shape (bs, n, d + 1)\n",
        "    '''\n",
        "    w = tf.reduce_any(samples, axis=-1)\n",
        "    w = w / tf.reduce_sum(w, axis=-1, keepdims=True)\n",
        "    return comp2dm(w, samples)\n",
        "\n",
        "def pure2dm(psi):\n",
        "    '''\n",
        "    Construct a factorized density matrix to represent a pure state\n",
        "    Arguments:\n",
        "     psi: tensor of shape (bs, d)\n",
        "    Returns:\n",
        "     dm: tensor of shape (bs, 1, d + 1)\n",
        "    '''\n",
        "    ones = tf.ones_like(psi[:, 0:1])\n",
        "    dm = tf.concat((ones[:,tf.newaxis, :],\n",
        "                    psi[:,tf.newaxis, :]),\n",
        "                   axis=2)\n",
        "    return dm\n",
        "\n",
        "def dm2discrete(dm):\n",
        "    '''\n",
        "    Creates a discrete distribution from the components of a density matrix\n",
        "    Arguments:\n",
        "     dm: tensor of shape (bs, n, d + 1)\n",
        "    Returns:\n",
        "     prob: vector of probabilities (bs, d)\n",
        "    '''\n",
        "    w, v = dm2comp(dm)\n",
        "    w = w / tf.reduce_sum(w, axis=-1, keepdims=True)\n",
        "    norms_v = tf.expand_dims(tf.linalg.norm(v, axis=-1), axis=-1)\n",
        "    v = v / norms_v\n",
        "    probs = tf.einsum('...j,...ji->...i', w, v ** 2, optimize=\"optimal\")\n",
        "    return probs\n",
        "\n",
        "def dm2distrib(dm, sigma):\n",
        "    '''\n",
        "    Creates a Gaussian mixture distribution from the components of a density\n",
        "    matrix with an RBF kernel\n",
        "    Arguments:\n",
        "     dm: tensor of shape (bs, n, d + 1)\n",
        "     sigma: sigma parameter of the RBF kernel\n",
        "    Returns:\n",
        "     gm: mixture of Gaussian distribution with shape (bs, )\n",
        "    '''\n",
        "    w, v = dm2comp(dm)\n",
        "    gm = tfd.MixtureSameFamily(reparameterize=True,\n",
        "            mixture_distribution=tfd.Categorical(\n",
        "                                    probs=w),\n",
        "            components_distribution=tfd.Independent( tfd.Normal(\n",
        "                    loc=v,  # component 2\n",
        "                    scale=sigma / np.sqrt(2.)),\n",
        "                    reinterpreted_batch_ndims=1))\n",
        "    return gm\n",
        "\n",
        "def pure_dm_overlap(x, dm, kernel):\n",
        "    '''\n",
        "    Calculates the overlap of a state  \\phi(x) with a density\n",
        "    matrix in a RKHS defined by a kernel\n",
        "    Arguments:\n",
        "      x: tensor of shape (bs, d)\n",
        "     dm: tensor of shape (bs, n, d + 1)\n",
        "     kernel: kernel function\n",
        "              k: (bs, d) x (bs, n, d) -> (bs, n)\n",
        "    Returns:\n",
        "     overlap: tensor with shape (bs, )\n",
        "    '''\n",
        "    w, v = dm2comp(dm)\n",
        "    overlap = tf.einsum('...i,...i->...', w, kernel(x, v) ** 2)\n",
        "    return overlap\n",
        "\n",
        "class CompTransKernelLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, transform, kernel):\n",
        "        '''\n",
        "        Composes a transformation and a kernel to create a new\n",
        "        kernel.\n",
        "        Arguments:\n",
        "            transform: a function f that transform the input before feeding it to the\n",
        "                    kernel\n",
        "                    f:(bs, d) -> (bs, D)\n",
        "            kernel: a kernel function\n",
        "                    k:(bs, n, D)x(m, D) -> (bs, n, m)\n",
        "        '''\n",
        "        super(CompTransKernelLayer, self).__init__()\n",
        "        self.transform = transform\n",
        "        self.kernel = kernel\n",
        "\n",
        "    def call(self, A, B):\n",
        "        '''\n",
        "        Input:\n",
        "            A: tensor of shape (bs, n, d)\n",
        "            B: tensor of shape (m, d)\n",
        "        Result:\n",
        "            K: tensor of shape (bs, n, m)\n",
        "        '''\n",
        "        shape = tf.shape(A) # (bs, n, d)\n",
        "        A = tf.reshape(A, [shape[0] * shape[1], shape[2]])\n",
        "        A = self.transform(A)\n",
        "        dim_out = tf.shape(A)[1]\n",
        "        A = tf.reshape(A, [shape[0], shape[1], dim_out])\n",
        "        B = self.transform(B)\n",
        "        return self.kernel(A, B)\n",
        "\n",
        "    def log_weight(self):\n",
        "        return self.kernel.log_weight()\n",
        "\n",
        "class RBFKernelLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, sigma, dim, trainable=True, min_sigma=1e-3):\n",
        "        '''\n",
        "        Builds a layer that calculates the rbf kernel between two set of vectors\n",
        "        Arguments:\n",
        "            sigma: RBF scale parameter. If it is a tf.Variable it will be used as is.\n",
        "                     Otherwise it will create a trainable variable with the given value.\n",
        "        '''\n",
        "        super(RBFKernelLayer, self).__init__()\n",
        "        if type(sigma) is tf.Variable:\n",
        "            self.sigma = sigma\n",
        "        else:\n",
        "            self.sigma = tf.Variable(sigma, dtype=tf.float32, trainable=trainable)\n",
        "        self.dim = dim\n",
        "        self.min_sigma = min_sigma\n",
        "\n",
        "    def call(self, A, B):\n",
        "        '''\n",
        "        Input:\n",
        "            A: tensor of shape (bs, n, d)\n",
        "            B: tensor of shape (m, d)\n",
        "        Result:\n",
        "            K: tensor of shape (bs, n, m)\n",
        "        '''\n",
        "        shape_A = tf.shape(A)\n",
        "        shape_B = tf.shape(B)\n",
        "        A_norm = tf.norm(A, axis=-1)[..., tf.newaxis] ** 2\n",
        "        B_norm = tf.norm(B, axis=-1)[tf.newaxis, tf.newaxis, :] ** 2\n",
        "        A_reshaped = tf.reshape(A, [-1, shape_A[2]])\n",
        "        AB = tf.matmul(A_reshaped, B, transpose_b=True)\n",
        "        AB = tf.reshape(AB, [shape_A[0], shape_A[1], shape_B[0]])\n",
        "        dist2 = A_norm + B_norm - 2. * AB\n",
        "        dist2 = tf.clip_by_value(dist2, 0., np.inf)\n",
        "        sigma = tf.clip_by_value(self.sigma, self.min_sigma, np.inf)\n",
        "        K = tf.exp(-dist2 / (2. * sigma ** 2.))\n",
        "        return K\n",
        "\n",
        "    def log_weight(self):\n",
        "        sigma = tf.clip_by_value(self.sigma, self.min_sigma, np.inf)\n",
        "        return - self.dim * tf.math.log(sigma + 1e-12) - self.dim * np.log(4 * np.pi)\n",
        "\n",
        "class CosineKernelLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Builds a layer that calculates the cosine kernel between two set of vectors\n",
        "        '''\n",
        "        super(CosineKernelLayer, self).__init__()\n",
        "        self.eps = 1e-6\n",
        "\n",
        "    def call(self, A, B):\n",
        "        '''\n",
        "        Input:\n",
        "            A: tensor of shape (bs, n, d)\n",
        "            B: tensor of shape (m, d)\n",
        "        Result:\n",
        "            K: tensor of shape (bs, n, m)\n",
        "        '''\n",
        "        A = tf.math.divide_no_nan(A,\n",
        "                                  tf.expand_dims(tf.norm(A, axis=-1), axis=-1))\n",
        "        B = tf.math.divide_no_nan(B,\n",
        "                                  tf.expand_dims(tf.norm(B, axis=-1), axis=-1))\n",
        "        K = tf.einsum(\"...nd,md->...nm\", A, B)\n",
        "        return K\n",
        "\n",
        "    def log_weight(self):\n",
        "        return 0\n",
        "\n",
        "class CrossProductKernelLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, dim1, kernel1, kernel2):\n",
        "        '''\n",
        "        Create a layer that calculates the cross product kernel of two input\n",
        "        kernels. The input vector are divided into two parts, the first of dimension\n",
        "        dim1 and the second of dimension d - dim1. Each input kernel is applied to\n",
        "        one of the parts of the input.\n",
        "        Arguments:\n",
        "            dim1: the dimension of the first part of the input vector\n",
        "            kernel1: a kernel function\n",
        "                    k1:(bs, n, dim1)x(m, dim1) -> (bs, n, m)\n",
        "            kernel2: a kernel function\n",
        "                    k2:(bs, n, d - dim1)x(m, d - dim1) -> (bs, n, m)\n",
        "        '''\n",
        "\n",
        "        super(CrossProductKernelLayer, self).__init__()\n",
        "        self.dim1 = dim1\n",
        "        self.kernel1 = kernel1\n",
        "        self.kernel2 = kernel2\n",
        "\n",
        "    def call(self, A, B):\n",
        "        '''\n",
        "        Input:\n",
        "            A: tensor of shape (bs, n, d)\n",
        "            B: tensor of shape (m, d)\n",
        "        Result:\n",
        "            K: tensor of shape (bs, n, m)\n",
        "        '''\n",
        "        A1 = A[:, :, :self.dim1]\n",
        "        A2 = A[:, :, self.dim1:]\n",
        "        B1 = B[:, :self.dim1]\n",
        "        B2 = B[:, self.dim1:]\n",
        "        return self.kernel1(A1, B1) * self.kernel2(A2, B2)\n",
        "\n",
        "    def log_weight(self):\n",
        "        return self.kernel1.log_weight() + self.kernel2.log_weight()\n",
        "\n",
        "def l1_loss(vals):\n",
        "    '''\n",
        "    Calculate the l1 loss for a batch of vectors\n",
        "    Arguments:\n",
        "        vals: tensor with shape (b_size, n)\n",
        "    '''\n",
        "    b_size = tf.cast(tf.shape(vals)[0], dtype=tf.float32)\n",
        "    vals = vals / tf.norm(vals, axis=1)[:, tf.newaxis]\n",
        "    loss = tf.reduce_sum(tf.abs(vals)) / b_size\n",
        "    return loss\n",
        "\n",
        "class KDMUnit(tf.keras.layers.Layer):\n",
        "    \"\"\"Kernel Quantum Measurement Unit\n",
        "    Receives as input a factored density matrix represented by a set of vectors\n",
        "    and weight values.\n",
        "    Returns a resulting factored density matrix.\n",
        "    Input shape:\n",
        "        (batch_size, n_comp_in, dim_x + 1)\n",
        "        where dim_x is the dimension of the input state\n",
        "        and n_comp_in is the number of components of the input factorization.\n",
        "        The weights of the input factorization of sample i are [i, :, 0],\n",
        "        and the vectors are [i, :, 1:dim_x + 1].\n",
        "    Output shape:\n",
        "        (batch_size, n_comp, dim_y)\n",
        "        where dim_y is the dimension of the output state\n",
        "        and n_comp is the number of components used to represent the train\n",
        "        density matrix. The weights of the\n",
        "        output factorization for sample i are [i, :, 0], and the vectors\n",
        "        are [i, :, 1:dim_y + 1].\n",
        "    Arguments:\n",
        "        dim_x: int. the dimension of the input state\n",
        "        dim_y: int. the dimension of the output state\n",
        "        x_train: bool. Whether to train or not the x compoments of the train\n",
        "                       density matrix.\n",
        "        x_train: bool. Whether to train or not the y compoments of the train\n",
        "                       density matrix.\n",
        "        w_train: bool. Whether to train or not the weights of the compoments\n",
        "                       of the train density matrix.\n",
        "        n_comp: int. Number of components used to represent\n",
        "                 the train density matrix\n",
        "        l1_act: float. Coefficient of the regularization term penalizing the l1\n",
        "                       norm of the activations.\n",
        "        l1_x: float. Coefficient of the regularization term penalizing the l1\n",
        "                       norm of the x components.\n",
        "        l1_y: float. Coefficient of the regularization term penalizing the l1\n",
        "                       norm of the y components.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            kernel,\n",
        "            dim_x: int,\n",
        "            dim_y: int,\n",
        "            x_train: bool = True,\n",
        "            y_train: bool = True,\n",
        "            w_train: bool = True,\n",
        "            n_comp: int = 0,\n",
        "            l1_x: float = 0.,\n",
        "            l1_y: float = 0.,\n",
        "            l1_act: float = 0.,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.kernel = kernel\n",
        "        self.dim_x = dim_x\n",
        "        self.dim_y = dim_y\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.w_train = w_train\n",
        "        self.n_comp = n_comp\n",
        "        self.l1_x = l1_x\n",
        "        self.l1_y = l1_y\n",
        "        self.l1_act = l1_act\n",
        "        self.c_x = self.add_weight(\n",
        "            \"c_x\",\n",
        "            shape=(self.n_comp, self.dim_x),\n",
        "            #initializer=tf.keras.initializers.orthogonal(),\n",
        "            initializer=tf.keras.initializers.random_normal(),\n",
        "            trainable=self.x_train)\n",
        "        self.c_y = self.add_weight(\n",
        "            \"c_y\",\n",
        "            shape=(self.n_comp, self.dim_y),\n",
        "            initializer=tf.keras.initializers.Constant(np.sqrt(1./self.dim_y)),\n",
        "            #initializer=tf.keras.initializers.random_normal(),\n",
        "            trainable=self.y_train)\n",
        "        self.comp_w = self.add_weight(\n",
        "            \"comp_w\",\n",
        "            shape=(self.n_comp,),\n",
        "            initializer=tf.keras.initializers.constant(1./self.n_comp),\n",
        "            trainable=self.w_train)\n",
        "        self.eps = 1e-10\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Weight regularizers\n",
        "        if self.l1_x != 0:\n",
        "            self.add_loss(self.l1_x * l1_loss(self.c_x))\n",
        "        if self.l1_y != 0:\n",
        "            self.add_loss(self.l1_y * l1_loss(self.c_y))\n",
        "        #comp_w = tf.clip_by_value(self.comp_w, 1e-10, 1)\n",
        "        comp_w = tf.abs(self.comp_w) + 1e-6\n",
        "        # normalize comp_w to sum to 1\n",
        "        comp_w = comp_w / tf.reduce_sum(comp_w)\n",
        "        in_w = inputs[:, :, 0]  # shape (b, n_comp_in)\n",
        "        in_v = inputs[:, :, 1:] # shape (b, n_comp_in, dim_x)\n",
        "        out_vw = self.kernel(in_v, self.c_x)  # shape (b, n_comp_in, n_comp)\n",
        "        out_w = (tf.expand_dims(tf.expand_dims(comp_w, axis=0), axis=0) *\n",
        "                 tf.square(out_vw)) # shape (b, n_comp_in, n_comp)\n",
        "        out_w = tf.maximum(out_w, self.eps) #########\n",
        "        # out_w_sum = tf.maximum(tf.reduce_sum(out_w, axis=2), self.eps)  # shape (b, n_comp_in)\n",
        "        out_w_sum = tf.reduce_sum(out_w, axis=2) # shape (b, n_comp_in)\n",
        "        out_w = out_w / tf.expand_dims(out_w_sum, axis=2)\n",
        "        out_w = tf.einsum('...i,...ij->...j', in_w, out_w, optimize=\"optimal\")\n",
        "                # shape (b, n_comp)\n",
        "        if self.l1_act != 0:\n",
        "            self.add_loss(self.l1_act * l1_loss(out_w))\n",
        "        out_w = tf.expand_dims(out_w, axis=-1) # shape (b, n_comp, 1)\n",
        "        out_y_shape = tf.shape(out_w) + tf.constant([0, 0, self.dim_y - 1])\n",
        "        out_y = tf.broadcast_to(tf.expand_dims(self.c_y, axis=0), out_y_shape)\n",
        "        out = tf.concat((out_w, out_y), 2)\n",
        "        return out\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            \"dim_x\": self.dim_x,\n",
        "            \"dim_y\": self.dim_y,\n",
        "            \"n_comp\": self.n_comp,\n",
        "            \"x_train\": self.x_train,\n",
        "            \"y_train\": self.y_train,\n",
        "            \"w_train\": self.w_train,\n",
        "            \"l1_x\": self.l1_x,\n",
        "            \"l1_y\": self.l1_y,\n",
        "            \"l1_act\": self.l1_act,\n",
        "        }\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, **config}\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (self.dim_y + 1, self.n_comp)\n",
        "\n",
        "class KDMOverlap(tf.keras.layers.Layer):\n",
        "    \"\"\"Kernel Quantum Measurement Overlap Unit\n",
        "    Receives as input a vector and calculates its overlap with the unit density\n",
        "    matrix.\n",
        "    Input shape:\n",
        "        (batch_size, dim_x)\n",
        "        where dim_x is the dimension of the input state\n",
        "    Output shape:\n",
        "        (batch_size, )\n",
        "    Arguments:\n",
        "        kernel: a kernel function\n",
        "        dim_x: int. the dimension of the input state\n",
        "        x_train: bool. Whether to train the or not the compoments of the train\n",
        "                       density matrix.\n",
        "        w_train: bool. Whether to train the or not the weights of the compoments\n",
        "                       of the train density matrix.\n",
        "        n_comp: int. Number of components used to represent\n",
        "                 the train density matrix\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            kernel,\n",
        "            dim_x: int,\n",
        "            x_train: bool = True,\n",
        "            w_train: bool = True,\n",
        "            n_comp: int = 0,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.kernel = kernel\n",
        "        self.dim_x = dim_x\n",
        "        self.x_train = x_train\n",
        "        self.w_train = w_train\n",
        "        self.n_comp = n_comp\n",
        "        self.c_x = self.add_weight(\n",
        "            \"c_x\",\n",
        "            shape=(self.n_comp, self.dim_x),\n",
        "            #initializer=tf.keras.initializers.orthogonal(),\n",
        "            initializer=tf.keras.initializers.random_normal(),\n",
        "            trainable=self.x_train)\n",
        "        self.comp_w = self.add_weight(\n",
        "            \"comp_w\",\n",
        "            shape=(self.n_comp,),\n",
        "            initializer=tf.keras.initializers.constant(1./self.n_comp),\n",
        "            trainable=self.w_train)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        #comp_w = tf.clip_by_value(self.comp_w, 1e-10, 1)\n",
        "        comp_w = tf.abs(self.comp_w)\n",
        "        # normalize comp_w to sum to 1\n",
        "        comp_w = comp_w / tf.reduce_sum(comp_w)\n",
        "        in_v = inputs[:, tf.newaxis, :]\n",
        "        out_vw = self.kernel(in_v, self.c_x) ** 2 # shape (b, 1, n_comp)\n",
        "        out_w = tf.einsum('...j,...ij->...', comp_w, out_vw, optimize=\"optimal\")\n",
        "        return out_w\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            \"dim_x\": self.dim_x,\n",
        "            \"n_comp\": self.n_comp,\n",
        "            \"x_train\": self.x_train,\n",
        "            \"w_train\": self.w_train,\n",
        "        }\n",
        "        base_config = super().get_config()\n",
        "        return {**base_config, **config}\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (1,)\n",
        "\n",
        "class KDMClassModel(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 dim_x,\n",
        "                 dim_y,\n",
        "                 sigma,\n",
        "                 n_comp,\n",
        "                 x_train=True):\n",
        "        super().__init__()\n",
        "        self.dim_x = dim_x\n",
        "        self.dim_y = dim_y\n",
        "        self.n_comp = n_comp\n",
        "        self.kernel_x = RBFKernelLayer(sigma, dim=dim_x)\n",
        "        self.kdmu = KDMUnit(self.kernel_x,\n",
        "                            dim_x=dim_x,\n",
        "                            dim_y=dim_y,\n",
        "                            n_comp=n_comp,\n",
        "                            x_train=x_train)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        rho_x = pure2dm(inputs)\n",
        "        rho_y = self.kdmu(rho_x)\n",
        "        probs = dm2discrete(rho_y)\n",
        "        return probs\n",
        "\n",
        "class BagKDMClassModel(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 dim_x,\n",
        "                 dim_y,\n",
        "                 sigma,\n",
        "                 n_comp,\n",
        "                 x_train=True,\n",
        "                 l1_y=0.):\n",
        "        super().__init__()\n",
        "        self.dim_x = dim_x\n",
        "        self.dim_y = dim_y\n",
        "        self.n_comp = n_comp\n",
        "        kernel_x = RBFKernelLayer(sigma)\n",
        "        self.kdmu = KDMUnit(kernel_x,\n",
        "                            dim_x=dim_x,\n",
        "                            dim_y=dim_y,\n",
        "                            n_comp=n_comp,\n",
        "                            x_train=x_train,\n",
        "                            l1_y=l1_y)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        in_shape = tf.shape(inputs)\n",
        "        w = tf.ones_like(inputs[:, :, 0]) / in_shape[1]\n",
        "        rho_x = comp2dm(w, inputs)\n",
        "        rho_y = self.kdmu(rho_x)\n",
        "        probs = dm2discrete(rho_y)\n",
        "        return rho_y\n",
        "\n",
        "class KDMDenEstModel(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 dim_x,\n",
        "                 sigma,\n",
        "                 n_comp):\n",
        "        super().__init__()\n",
        "        self.dim_x = dim_x\n",
        "        self.n_comp = n_comp\n",
        "        self.kernel = RBFKernelLayer(sigma, dim=dim_x)\n",
        "        self.kdmover = KDMOverlap(self.kernel,\n",
        "                                dim_x=dim_x,\n",
        "                                n_comp=n_comp)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        log_probs = (tf.math.log(self.kdmover(inputs) + 1e-12)\n",
        "                     + self.kernel.log_weight())\n",
        "        self.add_loss(-tf.reduce_mean(log_probs))\n",
        "        return log_probs\n",
        "\n",
        "class KDMDenEstModel2(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 dim_x,\n",
        "                 dim_y,\n",
        "                 sigma,\n",
        "                 n_comp,\n",
        "                 trainable_sigma=True,\n",
        "                 min_sigma=1e-3):\n",
        "        super().__init__()\n",
        "        self.dim_x = dim_x\n",
        "        self.dim_y = dim_y\n",
        "        self.n_comp = n_comp\n",
        "        self.kernel_x = RBFKernelLayer(sigma, dim=dim_x,\n",
        "                                       trainable=trainable_sigma,\n",
        "                                       min_sigma=min_sigma)\n",
        "        self.kernel_y = CosineKernelLayer()\n",
        "        self.kernel = CrossProductKernelLayer(dim1=dim_x, kernel1=self.kernel_x, kernel2=self.kernel_y)\n",
        "        self.kdmover = KDMOverlap(self.kernel,\n",
        "                                dim_x=dim_x + dim_y,\n",
        "                                n_comp=n_comp)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        log_probs = (tf.math.log(self.kdmover(inputs) + 1e-12)\n",
        "                     + self.kernel.log_weight())\n",
        "        self.add_loss(-tf.reduce_mean(log_probs))\n",
        "        return log_probs\n",
        "\n",
        "class KQClassBagModel(tf.keras.Model):\n",
        "    def __init__(self,\n",
        "                 encoded_size,\n",
        "                 dim_y,\n",
        "                 encoder,\n",
        "                 n_comp,\n",
        "                 sigma=0.1,\n",
        "                 mle_weight=0.):\n",
        "        super().__init__()\n",
        "        self.dim_y = dim_y\n",
        "        self.encoded_size = encoded_size\n",
        "        self.encoder = encoder\n",
        "        self.n_comp = n_comp\n",
        "        self.mle_weight = mle_weight\n",
        "        self.kernel = RBFKernelLayer(sigma=sigma,\n",
        "                                         dim=encoded_size,\n",
        "                                         trainable=True)\n",
        "        self.kdm_unit = KDMUnit(kernel=self.kernel,\n",
        "                                       dim_x=encoded_size,\n",
        "                                       dim_y=dim_y,\n",
        "                                       n_comp=n_comp)\n",
        "        self.regression_layer = ProbRegression()\n",
        "\n",
        "    def call(self, input):\n",
        "        encoded = self.encoder(input)\n",
        "        rho_x = pure2dm(encoded)\n",
        "        rho_y = self.kdm_unit(rho_x)\n",
        "        probs = dm2discrete(rho_y)\n",
        "        mean_var = self.regression_layer(probs)\n",
        "        return mean_var\n",
        "\n",
        "    def init_components(self, samples_x, samples_y, init_sigma=False, sigma_mult=1):\n",
        "        encoded_x = self.encoder(samples_x)\n",
        "        if init_sigma:\n",
        "            distances = pairwise_distances(encoded_x)\n",
        "            sigma = np.mean(distances) * sigma_mult\n",
        "            self.kernel.sigma.assign(sigma)\n",
        "        self.kdm_unit.c_x.assign(encoded_x)\n",
        "        self.kdm_unit.c_y.assign(samples_y)\n",
        "        self.kdm_unit.comp_w.assign(tf.ones((self.n_comp,)) / self.n_comp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22MkJMvoc9aa"
      },
      "source": [
        "##  Create encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxY4SoL_bE5b"
      },
      "outputs": [],
      "source": [
        "encoded_size = 128\n",
        "\n",
        "def create_convnext_encoder(encoded_size):\n",
        "  convnext = tf.keras.applications.convnext.ConvNeXtTiny(\n",
        "    model_name='convnext_tiny',\n",
        "    include_top=False,\n",
        "    include_preprocessing=True,\n",
        "    weights='imagenet',\n",
        "    input_tensor=None,\n",
        "    input_shape=(256,256,3),\n",
        "    pooling=\"avg\",\n",
        "    classes=5,\n",
        "    classifier_activation='softmax'\n",
        "  )\n",
        "  encoder = keras.Sequential([\n",
        "      Input(shape=(256, 256, 3)),\n",
        "      convnext,\n",
        "      keras.layers.Dropout(0.2),\n",
        "      keras.layers.Dense(encoded_size, activation=\"tanh\"), #relu, linear\n",
        "  ])\n",
        "\n",
        "  encoder_cls = keras.Sequential([encoder,\n",
        "                                keras.layers.Dense(5, activation=\"softmax\")],\n",
        "  )\n",
        "  return encoder, encoder_cls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JHftzXgcR7S",
        "outputId": "e98a2f46-5a5a-4040-dcc9-d05e11f636b7"
      },
      "outputs": [],
      "source": [
        "#encoder, encoder_cls = create_convnext_encoder(encoded_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auQQNnijdH2X"
      },
      "source": [
        "## ConvNeXT Training / Warmup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alpha = 0.1\n",
        "def loss(y_true, y_pred):\n",
        "  return tf.keras.losses.mean_squared_error(y_true, y_pred[:,0:1])  +  alpha * y_pred[:, 1:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_avt12EdjmQ"
      },
      "outputs": [],
      "source": [
        "# checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "#     \"/home/smedin7/data/KDM/models/patch_regression_weights.h5\",\n",
        "#     monitor = \"val_cohen_kappa\",\n",
        "#     verbose = 1,\n",
        "#     save_best_only = True,\n",
        "#     save_weights_only = True,\n",
        "#     mode = \"max\",\n",
        "#     save_freq=\"epoch\",\n",
        "# )\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode=\"min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr1jb0CccW_i"
      },
      "outputs": [],
      "source": [
        "# encoder_cls.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "#                 loss=losses.categorical_crossentropy,\n",
        "#                 metrics=[metrics.categorical_accuracy, tfa.metrics.CohenKappa(num_classes = 5, weightage='quadratic')], loss_weights=class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjlIraVZeP52"
      },
      "outputs": [],
      "source": [
        "# encoder_cls.load_weights('/content/drive/MyDrive/data/kdm_data/best_convnext_patch_classifier.h5')\n",
        "# encoder.save_weights('/content/drive/MyDrive/data/kdm_data/best_convnext_patch_encoder.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ial0aBu2dBvm",
        "outputId": "a061a94f-ef7b-4e11-ad7b-157bfc29bfef"
      },
      "outputs": [],
      "source": [
        "# history = encoder_cls.fit(train_dataset, validation_data=val_dataset, epochs=1, verbose=1, callbacks=[checkpoint_callback, early_stopping, WandbMetricsLogger(log_freq='epoch')])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4IbjoOQeTln",
        "outputId": "62b00557-0865-4db9-ec27-d52e56863cba"
      },
      "outputs": [],
      "source": [
        "# encoder_cls.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkWRj9hGlQuE"
      },
      "source": [
        "Confusion matrix of convnext alone to classify patches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvYslZZhjI_E",
        "outputId": "b82a0881-ef45-4f08-f6da-66568f24895b"
      },
      "outputs": [],
      "source": [
        "# preds = encoder_cls.predict(test_dataset)\n",
        "# predictions = np.argmax(preds, axis =1)\n",
        "# y_true = np.argmax(np.concatenate([y for x,y in test_dataset], axis=0), axis = 1)\n",
        "# ConfusionMatrixDisplay.from_predictions(y_true, predictions, normalize='true', display_labels=['Stroma', 'Healthy', 'G 3', 'G 4', 'G 5'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9IojboTdFvd"
      },
      "source": [
        "## KDM Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDiX24Uygj_x"
      },
      "source": [
        "#### Create encoder for KDM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02Ds1wmrTFkK"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U23tFEP6dbEE"
      },
      "outputs": [],
      "source": [
        "kdm_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    \"/data/KDM/models/patch_regression_weights.h5\",\n",
        "    monitor = \"val_mean_absolute_error\",\n",
        "    verbose = 1,\n",
        "    save_best_only = True,\n",
        "    save_weights_only = True,\n",
        "    mode = \"min\",\n",
        "    save_freq=\"epoch\",\n",
        ")\n",
        "\n",
        "early_stopping_callback = EarlyStopping(\n",
        "    monitor='val_mean_absolute_error',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    mode='min',\n",
        "    restore_best_weights=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkC6YBoBgncU"
      },
      "source": [
        "### Define number of components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyoZudVWdOf5"
      },
      "outputs": [],
      "source": [
        "n_comp = 216"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJeYkEyVgqM7"
      },
      "source": [
        "### Instantiate model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bJvYNlRdWYl"
      },
      "source": [
        "### Initialize KDM with prototypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg4o6_apPTi0"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "57daafd0bae14951aea1eedf4c9a4153",
            "87766424ad3347acb77b19b4e51d0f77",
            "b4d3c2d8085a4b8ea2db76725758700f",
            "d060e5edc5e647ccaeb5089af08b2b91",
            "7c0cf808911f4a76a536c3e4f69a45b0",
            "366b2f51442347aba1b2c4804ff08580",
            "9c4b42e1ae014bb18e52a9882d5c204e",
            "c2dd7e93581a47f987b329d18a9cc2c4",
            "f440c0fb4e4843c59db5ac26996fe269",
            "cddbd73a00274cc78b9b9ed935de1c55",
            "7975a8a1cdd04c73937ef567dd521743"
          ]
        },
        "id": "W_4xwAdh5-ct",
        "outputId": "4f92c4fc-e2e4-41c7-9125-6f677d518657"
      },
      "outputs": [],
      "source": [
        "mirrored_strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with mirrored_strategy.scope():\n",
        "    encoder_kdm, _ = create_convnext_encoder(encoded_size)\n",
        "    encoder_kdm.load_weights('/data/KDM/models/best_convnext_patch_encoder.h5')\n",
        "    kdm_cls = KQClassBagModel(\n",
        "                    encoded_size=encoded_size,\n",
        "                    dim_y=5,\n",
        "                    encoder=encoder_kdm,\n",
        "                    n_comp=n_comp,\n",
        "                    sigma=0.1)\n",
        "    kdm_cls(next(iter(train_dataset))[0])\n",
        "    # patch_protos, patch_label_protos = get_patch_prototypes()\n",
        "    # kdm_cls.init_components(patch_protos, patch_label_protos, init_sigma = True, sigma_mult = 1.)\n",
        "    kdm_cls.load_weights(\"/data/KDM/models/patch_regression_weights.h5\")\n",
        "    kdm_cls.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "                    loss=loss,\n",
        "                    metrics=[metrics.mean_absolute_error,\n",
        "                            tfa.metrics.CohenKappa(num_classes = 5, weightage='quadratic')])\n",
        "print(kdm_cls.kernel.sigma.numpy())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CBaMGlSQdipO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fci9j3spmcj_"
      },
      "outputs": [],
      "source": [
        "# _ = kdm_cls(tf.zeros((1, 256,256,3)))\n",
        "# kdm_cls.load_weights('/content/drive/MyDrive/data/kdm_data/best_kdm_patch_convnext.h5')\n",
        "#encoder_kdm.save_weights('/content/drive/MyDrive/data/kdm_data/best_kdm_patch_convnext_extractor.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a20IGIaydlaT"
      },
      "outputs": [],
      "source": [
        "history = kdm_cls.fit(train_dataset, validation_data=val_dataset, epochs=30, verbose=1, callbacks=[kdm_checkpoint_callback, early_stopping_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP0qkByvPO6_",
        "outputId": "fa923d1d-6b56-43df-aaf5-313dce556af0"
      },
      "outputs": [],
      "source": [
        "kdm_cls.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "out = kdm_cls.predict(test_dataset)\n",
        "y_pred, std = out[:, 0], np.sqrt(out[:, 1])\n",
        "\n",
        "predictions = np.round(y_pred * 4)\n",
        "y_true = np.round(np.concatenate([y for x,y in test_dataset], axis=0) * 4)\n",
        "\n",
        "print(ConfusionMatrixDisplay.from_predictions(y_true, predictions, normalize='true', display_labels=['Stroma', 'Healthy', 'G3', 'G 4', 'G 5']))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_true_reg = np.round(np.concatenate([y for x,y in test_dataset], axis=0))\n",
        "\n",
        "print(mean_absolute_error(y_true_reg, y_pred))\n",
        "\n",
        "print(cohen_kappa_score(y_true, predictions, weights='quadratic'))\n",
        "\n",
        "print(classification_report(y_true, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "var = std ** 2\n",
        "abs_errors = np.abs(predictions - y_true)\n",
        "\n",
        "# Group predictions and variance by absolute error\n",
        "data_to_plot = {'Error Group': [], 'Variance': [], 'Predictions': []}\n",
        "for i, error in enumerate(abs_errors):\n",
        "    if error == 0:\n",
        "        error_group = '0'\n",
        "    elif error == 1:\n",
        "        error_group = '1'\n",
        "    else:\n",
        "        error_group = '2+'\n",
        "\n",
        "    data_to_plot['Error Group'].append(error_group)\n",
        "    data_to_plot['Variance'].append(var[i])\n",
        "    data_to_plot['Predictions'].append(predictions[i])\n",
        "\n",
        "# Create a DataFrame\n",
        "df = pd.DataFrame(data_to_plot)\n",
        "\n",
        "# Get the counts for each error group\n",
        "counts = df['Error Group'].value_counts().loc[['0', '1', '2+']]\n",
        "# Define the color palette\n",
        "palette = sns.color_palette(\"Pastel1\", n_colors=3)\n",
        "sample_fraction = 0.005  # 10% of data\n",
        "df_sampled = df.groupby('Error Group').apply(lambda x: x.sample(frac=sample_fraction)).reset_index(drop=True)\n",
        "\n",
        "# Plot the variance using seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(x='Error Group', y='Variance', data=df, order=['0', '1', '2+'], palette=palette)\n",
        "sns.stripplot(x='Error Group', y='Variance', data=df_sampled, jitter=True, marker=\"o\", alpha=0.2, color='gray')\n",
        "\n",
        "plt.title('Variance by Absolute Error Group', fontsize=15)\n",
        "plt.xlabel('Error Group', fontsize=13)\n",
        "plt.ylabel('Variance', fontsize=13)\n",
        "\n",
        "# Create custom legend\n",
        "legend_labels = [f'{key}: {value} samples' for key, value in counts.items()]\n",
        "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=palette[i], markersize=10) for i in range(3)]\n",
        "plt.legend(legend_handles, legend_labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "plt.savefig('variance_plot.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n",
        "\n",
        "# Plot the predictions using seaborn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(x='Error Group', y='Predictions', data=df, order=['0', '1', '2+'], palette=palette)\n",
        "\n",
        "\n",
        "# Create the strip plot using the sampled DataFrame\n",
        "sns.stripplot(x='Error Group', y='Predictions', data=df_sampled, jitter=True, marker=\"o\", alpha=0.2, color='gray')\n",
        "#sns.stripplot(x='Error Group', y='Predictions', data=df, jitter=True, marker=\"o\", alpha=0.4, color='black')\n",
        "\n",
        "plt.title('Predictions by Absolute Error Group', fontsize=15)\n",
        "plt.xlabel('Error Group', fontsize=13)\n",
        "plt.ylabel('Predictions', fontsize=13)\n",
        "\n",
        "# Create custom legend\n",
        "plt.legend(legend_handles, legend_labels, loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "plt.savefig('predictions_plot.png', bbox_inches='tight', dpi=600)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## testing model when taking into account variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "threshold = 0.05\n",
        "indices = np.where(var < threshold)[0]\n",
        "confident_predictions = predictions[indices]\n",
        "ground_truth_subset = y_true[indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(ConfusionMatrixDisplay.from_predictions(ground_truth_subset, confident_predictions, normalize='true', display_labels=['Stroma', 'Healthy', 'G3', 'G 4', 'G 5']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_true_reg_subset =  y_true_reg[indices]\n",
        "y_pred_subset = y_pred[indices]\n",
        "\n",
        "print(\"MAE: \",mean_absolute_error(y_true_reg_subset, y_pred_subset))\n",
        "\n",
        "print(\"KAPPA: \",cohen_kappa_score(ground_truth_subset, confident_predictions, weights='quadratic'))\n",
        "\n",
        "print(classification_report(ground_truth_subset, confident_predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "howE8skWOjIv"
      },
      "source": [
        "## extract features and compare with prototypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwvQPBo-Lx1b"
      },
      "source": [
        "### Load pretrained KDM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n84LgSI_LxKQ"
      },
      "outputs": [],
      "source": [
        "kdm_cls = KQClassBagModel(\n",
        "                 encoded_size=encoded_size,\n",
        "                 dim_y=5,\n",
        "                 encoder=encoder_kdm,\n",
        "                 n_comp=n_comp,\n",
        "                 sigma=0.1)\n",
        "kdm_cls.compile(optimizer=optimizers.Adam(learning_rate=1e-4),\n",
        "                loss=losses.categorical_crossentropy,\n",
        "                metrics=[metrics.categorical_accuracy,\n",
        "                         tfa.metrics.CohenKappa(num_classes = 5, weightage='quadratic')])\n",
        "_ = kdm_cls(tf.zeros((1, 256,256,3)))\n",
        "kdm_cls.load_weights('/content/drive/MyDrive/data/kdm_data/best_kdm_patch_convnext.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BK8K06hLSSH",
        "outputId": "02ae5f77-5a54-4985-b9e8-6608b5a28409"
      },
      "outputs": [],
      "source": [
        "#features = encoder_kdm.predict(train_dataset)\n",
        "np.save(\"/content/drive/MyDrive/data/kdm_data/train_patch_features_kdm.npy\", features)\n",
        "c_x = kdm_cls.kdm_unit.c_x\n",
        "c_y = kdm_cls.kdm_unit.c_y\n",
        "np.save(\"/content/drive/MyDrive/data/kdm_data/c_x_features_kdm.npy\", c_x)\n",
        "np.save(\"/content/drive/MyDrive/data/kdm_data/c_y_features_kdm.npy\", c_y)\n",
        "indices = np.argmin(dist, axis = 0)\n",
        "comp_w = kdm_cls.kdm_unit.comp_w.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "XfV-Z99F9OPq",
        "outputId": "a6610724-eefc-43d6-ace1-61003ff5464f"
      },
      "outputs": [],
      "source": [
        "plt.plot(comp_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uXTF9m39mjX",
        "outputId": "922be90b-c295-41fc-b0ef-501a1a9873c8"
      },
      "outputs": [],
      "source": [
        "c_y[comp_w > 0.001]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyU2ifIQ90jI"
      },
      "outputs": [],
      "source": [
        "weighted_c_x = c_x[comp_w > 0.001]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kvSQU-8VRof"
      },
      "outputs": [],
      "source": [
        "dist = pairwise_distances(features, weighted_c_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSQNf51T-TM2"
      },
      "outputs": [],
      "source": [
        "indices = np.argmin(dist, axis = 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mFn3WbB3j_C",
        "outputId": "6b5cfa3d-d3fa-4a7b-b365-916ea031b1e0"
      },
      "outputs": [],
      "source": [
        "indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jimCUldy3299"
      },
      "outputs": [],
      "source": [
        "prototype_patches = df_train.iloc[indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwhTjPne4cnW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKIk2tzS4S1Z"
      },
      "outputs": [],
      "source": [
        "learned_prototype_labels = np.argmax(c_y[comp_w > 0.001], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab3mmOlh4k7S",
        "outputId": "bd754678-917c-4163-cf85-6ac64b35ca4e"
      },
      "outputs": [],
      "source": [
        "learned_prototype_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "re2xgKP84M7p"
      },
      "outputs": [],
      "source": [
        "img_ids = prototype_patches['image_id']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7r8aLaDr6az9"
      },
      "outputs": [],
      "source": [
        "def plot_patches(prototype_patches, gleason_score):\n",
        "  gleason_patches = prototype_patches[prototype_patches['gleason_score'] == gleason_score]\n",
        "  patches_path = gleason_patches['image_id'].apply(lambda x: os.path.join(\"/content/patches\", x.split('_')[0], x + '.jpeg'))\n",
        "  for i, path in enumerate(patches_path):\n",
        "    #plt.subplot(1, len(patches_path), i + 1)\n",
        "    patch = Image.open(path)\n",
        "    plt.imshow(patch)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        },
        "id": "ah7xLblp7Oi7",
        "outputId": "3ad857ed-65fa-4d2c-c13f-aa9c8b6f69dd"
      },
      "outputs": [],
      "source": [
        "plot_patches(prototype_patches,4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dsr8fDRN5fa2"
      },
      "outputs": [],
      "source": [
        "os.path.join(data_dir, x.split('_')[0], x + '.jpeg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svWUByzuLr_K",
        "outputId": "5d1883a7-4f47-4aa3-e5aa-01440421acda"
      },
      "outputs": [],
      "source": [
        "features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "zTJ6Q-OpTjht",
        "outputId": "e8d01821-91fb-4486-82ca-84b3ef1c702e"
      },
      "outputs": [],
      "source": [
        "preds = kdm_cls.predict(test_dataset)\n",
        "predictions = np.argmax(preds, axis =1)\n",
        "y_true = np.argmax(np.concatenate([y for x,y in test_dataset], axis=0), axis = 1)\n",
        "ConfusionMatrixDisplay.from_predictions(y_true, predictions, normalize='true', display_labels=['Stroma', 'Healthy', 'G 3', 'G 4', 'G 5'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPFdeWR6WEP5",
        "outputId": "4cf1fd11-e37e-476f-8363-bb583182c0a1"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true, predictions))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "b51ZIAmFekaG",
        "outputId": "96b1cd2a-e24e-4eb4-d6a5-723a6c1def14"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3VeIGaokV70"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "DHQHAECwfcYY",
        "gge1NlEVg-A2",
        "JSrOsHa2c47b",
        "auQQNnijdH2X"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "366b2f51442347aba1b2c4804ff08580": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57daafd0bae14951aea1eedf4c9a4153": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_87766424ad3347acb77b19b4e51d0f77",
              "IPY_MODEL_b4d3c2d8085a4b8ea2db76725758700f",
              "IPY_MODEL_d060e5edc5e647ccaeb5089af08b2b91"
            ],
            "layout": "IPY_MODEL_7c0cf808911f4a76a536c3e4f69a45b0"
          }
        },
        "7975a8a1cdd04c73937ef567dd521743": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c0cf808911f4a76a536c3e4f69a45b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87766424ad3347acb77b19b4e51d0f77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_366b2f51442347aba1b2c4804ff08580",
            "placeholder": "​",
            "style": "IPY_MODEL_9c4b42e1ae014bb18e52a9882d5c204e",
            "value": "100%"
          }
        },
        "9c4b42e1ae014bb18e52a9882d5c204e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4d3c2d8085a4b8ea2db76725758700f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2dd7e93581a47f987b329d18a9cc2c4",
            "max": 43,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f440c0fb4e4843c59db5ac26996fe269",
            "value": 43
          }
        },
        "c2dd7e93581a47f987b329d18a9cc2c4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cddbd73a00274cc78b9b9ed935de1c55": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d060e5edc5e647ccaeb5089af08b2b91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cddbd73a00274cc78b9b9ed935de1c55",
            "placeholder": "​",
            "style": "IPY_MODEL_7975a8a1cdd04c73937ef567dd521743",
            "value": " 43/43 [07:19&lt;00:00, 10.41s/it]"
          }
        },
        "f440c0fb4e4843c59db5ac26996fe269": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
